---
title: "BDA Coursework 2"
author: "Greta Sivickaite (ID 13168266)"
subtitle: "MSc Data Science (PT)"
date: "Sunday, January 12, 2020"
output: pdf_document
---
```{r setup, include=FALSE}
library(knitr)
library(boot)
library(tree)
library(randomForest)
library(ISLR)
library(ROCR)
library(MASS)
library(e1071)
opts_chunk$set(echo = TRUE)
```
<br></br>

### 1. Bayesian Networks and Naïve Bayes Classifiers  

(a) **Given a training dataset including 30 instances and a Bayesian network indicating the relationships between 3 features (i.e. Income, Student and Credit Rate), and the class attribute (i.e. Buy Computer), please create the conditional probability tables by hand.** 

```{r}

credit_rating_cpt <- data.frame('Buy computer' =
                            c('Yes','Yes','Yes','Yes','No','No','No','No'),
                          'Income' = 
                            c('Low','Low','High','High','Low','Low','High','High'),
                          'Student' = 
                            c('True','False','True','False','True','False','True','False'),
                          'Credit Rating is Fair' = 
                            c(3/5,2/4,1/2,1/3,5/7,1/2,2/4,1/3),
                          'Credit Rating is Excellent' = 
                            c(2/5,2/4,1/2,2/3,2/7,1/2,2/4,2/3)
                          )

student_cpt <- data.frame('Buy computer' =
                            c('Yes','No'),
                          'Student is True' = 
                            c(7/14,11/16),
                          'Student is False' = 
                            c(7/14,5/16)
                          )

income_cpt <- data.frame('Buy computer' =
                            c('Yes','No'),
                          'Income is High' = 
                            c(5/14,7/16),
                          'Income is Low' = 
                            c(9/14,9/16)
                          )

buy_computer_cpt <- data.frame('Buy computer is Yes' = 14/30,
                                'Buy computer is No' = 16/30
                              )

kable(credit_rating_cpt, 
      caption = "Conditional probability table for Credit rating feature")

kable(student_cpt, 
      caption = "Conditional probability table for Student feature")

kable(income_cpt, 
      caption = "Conditional probability table for Income feature")

kable(buy_computer_cpt, 
      caption = "Conditional probability table for Buy computer attribute")
```  

(b) **Make predictions for 2 testing instances by using the Bayesian network classifier.**

1) Income = Low, Student = False, Credit rating = Excellent. Prediction: Buy computer = Yes:       

* $P(Buy computer = Yes | Income = Low, Student = False, Credit rating = Excellent)= P(Credit rating = Excellent |Student = False, Income = Low, Buy computer = Yes)* P(Student = False | Buy computer = Yes) * P(Income = Low | Buy computer = Yes) * P(Buy computer = Yes) = 0.5 * 0.5 * 0.5625 * 0.4666667 = 0.065625$         
* $P(Buy computer = No | Income = Low, Student = False, Credit rating = Excellent) = P(Credit rating = Excellent |Student = False, Income = Low, Buy computer = No)* P(Student = False | Buy computer = No) * P(Income = Low | Buy computer = No) * P(Buy computer = No) = 0.5 * 0.3125 * 0.5625 * 0.5333333 = 0.03571428$        

<br></br>  

2) Income = High, Student = False, Credit rating = Fair. Prediction: Buy computer = Yes:     

* $P(Buy computer = Yes | Income = High, Student = False, Credit rating = Fair)= P(Credit rating = Fair |Student = False, Income = High, Buy computer = Yes)* P(Student = False | Buy computer = Yes) * P(Income = High | Buy computer = Yes) * P(Buy computer = Yes) = 0.3333333	* 0.5000 * 0.3571429 * 0.4666667 = 0.02777778$       
* $P(buy computer = No | Income = High, Student = False, Credit rating = Fair)= P(Credit rating = Fair |Student = False, Income = High, Buy computer = No)* P(Student = False | Buy computer = No) * P(Income = High | Buy computer = No) * P(Buy computer = No) = 0.3333333 * 0.3125 * 0.4375 * 0.5333333 = 0.02430555$     


(c) **Based on the conditional independence assumption between features, please create the conditional probability tables by hand.**  

```{r}

credit_rating_naive_cpt <- data.frame('Buy computer' =
                            c('Yes', 'No'),
                          'Credit Rating is Fair' = 
                            c(7/14,9/16),
                          'Credit Rating is Excellent' = 
                            c(7/14,7/16)
                          )

student_naive_cpt <- data.frame('Buy computer' =
                            c('Yes','No'),
                          'Student is True' = 
                            c(7/14,11/16),
                          'Student is False' = 
                            c(7/14,5/16)
                          )

income_naive_cpt <- data.frame('Buy computer' =
                            c('Yes','No'),
                          'Income is High' = 
                            c(5/14,7/16),
                          'Income is Low' = 
                            c(9/14,9/16)
                          )

buy_computer_naive_cpt <- data.frame('Buy computer is Yes' = 14/30,
                                'Buy computer is No' = 16/30
                              )

kable(credit_rating_naive_cpt, 
      caption = "Naive Conditional probability table for Credit rating feature")

kable(student_naive_cpt, 
      caption = "Naive Conditional probability table for Student feature")

kable(income_naive_cpt, 
      caption = "Naive Conditional probability table for Income feature")

kable(buy_computer_naive_cpt, 
      caption = "Naive Conditional probability table for Buy computer attribute")
```  

(d) **Make predictions for 2 testing instances by using the naïve Bayes classifier.**     
1) Income = Low, Student = False, Credit rating = Excellent. Prediction: Buy computer = Yes:       

* $P(Buy computer = Yes | Income = Low, Student = False, Credit rating = Excellent)= P(Credit rating = Excellent | Buy computer = Yes)* P(Student = False | Buy computer = Yes) * P(Income = Low | Buy computer = Yes) * P(Buy computer = Yes) = 0.5 * 0.5 * 0.6428571 * 0.4666667  = 0.075$         
* $P(Buy computer = No | Income = Low, Student = False, Credit rating = Excellent) = 0.4375 * 0.3125 * 0.6428571 * 0.5333333 = 0.04687499$         

<br></br>  

2) Income = High, Student = False, Credit rating = Fair. Prediction: Buy computer = Yes:     

* $P(Buy computer = Yes | Income = High, Student = False, Credit rating = Fair)= P(Credit rating = Fair | Buy computer = Yes)* P(Student = False | Buy computer = Yes) * P(Income = High | Buy computer = Yes) * P(Buy computer = Yes) = 0.5000 * 0.5000 * 0.3571429 * 0.4666667 = 0.04166667$          
* $P(buy computer = No | Income = High, Student = False, Credit rating = Fair)= P(Credit rating = Fair | Buy computer = No)* P(Student = False | Buy computer = No) * P(Income = High | Buy computer = No) * P(Buy computer = No) = 0.5625 * 0.3125 * 0.4375 * 0.5333333 = 0.04101562$       

<br></br>

### 2. Decision Trees and Random Forests

(a) **Load the room occupancy data and train a decision tree classifier. Evaluate the predictive performance by reporting the accuracy obtained on the testing dataset.**

```{r}
#Loading data sets
ro_training <- read.table("RoomOccupancy_Training.txt",
               header = TRUE,
               sep=",", 
               fill=FALSE, 
               strip.white=TRUE)

ro_testing <- read.table("RoomOccupancy_Testing.txt",
              header = TRUE,
              sep=",", 
              fill=FALSE, 
              strip.white=TRUE)

#Training a decision tree
ro_d_tree <- tree(Occupancy~., ro_training)


#Predictive performance
ro_d_tree_pred <- predict(ro_d_tree, ro_testing, type="class")
ro_d_prediction_table <- table(ro_d_tree_pred, ro_testing$Occupancy)
kable(ro_d_prediction_table)  

```

The error rate is 20.3%.

<br></br>

(b) **Output and analyse the tree learned by the decision tree algorithm, i.e. plot the tree structure and make a discussion about it.**

```{r}
#Summary of the decision tree
summary(ro_d_tree)

#Plotting the tree
plot(ro_d_tree)
text(ro_d_tree)
```
<br></br>

The tree consists of 6 terminal nodes and 10 branches. Out of 5 variables 4 were used to build the tree - `Light`,       `Temperature`, `CO2`, and `Humidity`.  The most important determinator is `Light`. The training error rate is 1%.

<br></br>

(c) **Train a random forests classifier, and evaluate the predictive performance by reporting the accuracy obtained on the testing dataset.**

```{r}
set.seed(5)

#Training a random forest
ro_rf <- randomForest(Occupancy ~ ., ro_training, mtry=5, importance = TRUE)

#Predictive performance
ro_rf_prediction <- predict(ro_rf, ro_testing)
ro_rf_prediction_table <- table(ro_rf_prediction, ro_testing$Occupancy)
kable(ro_rf_prediction_table)  

```

The error rate is 25.6%.

<br></br>

(d) **Output and analyse the feature importance obtained by the random forests classifier.**
```{r}
varImpPlot(ro_rf,
           main = 'Analysis of the random forest')
```

<br></br>

Based on random forests classifier, we see that `Light` is the most significant feature.

<br></br>


### 3. SVM

(a) **Download the wine quality data and use the training dataset to conduct the grid-search to find the optimal hyperparameters of svm by using the linear kernal.**  

```{r}
#Read text files with training and testing data
wq_training <- read.table("WineQuality_training.txt",
               header = TRUE,
               sep=",", 
               fill=FALSE, 
               strip.white=TRUE)

wq_testing <- read.table("WineQuality_testing.txt",
               header = TRUE,
               sep=",", 
               fill=FALSE, 
               strip.white=TRUE)

#Choose the optimal hyperparameters (linear kernal)
set.seed(1)
tune_linear <-tune(svm, 
                   quality ~ .,
                   data = wq_training, 
                   kernel = "linear",
                   ranges =list(cost=c(0.01, 0.1, 1, 5, 10)))

summary(tune_linear)

```

<br></br>

(b) **Train a svm classifier by using the linear kernal and the corresponding optimal hyperparameters, then make predictions on the testing dataset, report the predictive performance.**

```{r}
#Prediction on testing data set
wq_linear_prediction = predict(tune_linear$best.model, wq_testing)

#Predictive performance
wq_linear_performance = (table(predict = wq_linear_prediction, 
                               truth = wq_testing$quality))
kable(wq_linear_performance)
```

<br></br>

Based on the results above, we see that 104+169 observations out of 400 were correctly classified, which in turn suggests 68% correct predictive accuracy.

<br></br>

(c) **Conduct the grid-search to find the optimal hyperparameters of svm by using the RBF kernal.** 

```{r}
#Choose the optimal hyperparameters (RBF kernal)
set.seed(1)
tune_radial <-tune(svm, 
                   quality ~ .,
                   data = wq_training, 
                   kernel = "radial",
                   ranges =list(cost=c(0.01, 0.1, 1, 5, 10), 
                                gamma =c(0.01, 0.03, 0.1, 0.5, 1)))

summary(tune_radial)
```
<br></br>

(d) **Train a svm classifier by using the RBF kernal and the corresponding optimal hyperparameters, then make predictions on the testing dataset, report the predictive performance.**

```{r}
#Prediction on testing data set
wq_radial_prediction = predict(tune_radial$best.model, wq_testing)

#Predictive performance
wq_radial_performance = (table(predict = wq_radial_prediction, 
                               truth = wq_testing$quality))
kable(wq_radial_performance)  
```

Based on the results above, we see that 118+137 observations out of 400 were correctly classified, which in turn suggests 64% correct predictive accuracy.

<br></br>

(e) **Conduct the ROC curve analysis to compare the predictive performance of svm classifiers trained by using the linear and RBF kernels respectively.**

```{r}
#Plot ROC curve function (based on 'An Introduction to Statistical Learning  with Applications in R' by James et all, 2015)
rocplot=function(pred, truth, ...){
  predob = prediction(pred, truth)
  perf= performance(predob, "tpr", "fpr")
  plot(perf, ...)
}

#Asign decision values
fitted_radial <- attributes(predict(tune_radial$best.model, 
                                    wq_testing, 
                                    decision.values = TRUE))$decision.values
fitted_linear <- attributes(predict(tune_linear$best.model, 
                                    wq_testing, 
                                    decision.values = TRUE))$decision.values

#Plotting the curves for linear and RBF kernels
rocplot(fitted_radial, wq_testing$quality, main = "ROC curve analysis", col = 'Green')
rocplot(fitted_linear, wq_testing$quality, add = TRUE, col = 'Red')

legend(0.5,0.2, 
      legend = c('Linear',
                 'Radial'),
                lty=1, 
                cex=0.9, 
                bty="n", 
                col = c("red", "green"))

abline(a = 0, b = 1)
```

<br></br>

Based on the graph above, one can assume that the area under the curve is larger for Linear kernel, and therefore SVM of linear kernel has better performance capabilities.

<br></br>


### 4. Hierarchical Clustering

(a) **Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states.**
```{r}
usa_complete <- hclust(dist(USArrests), method = "complete")

plot(usa_complete)
```
<br></br>

(b) **Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?**
```{r}
#Cutting the tree
usa_three <- cutree(usa_complete, 3)

#Printing the first cluster states
kable(rownames(USArrests)[usa_three == 1],
      col.names = 'States',
      caption = 'First cluster')

#Printing the second cluster states
kable(rownames(USArrests)[usa_three == 2],
      col.names = 'States',
      caption = 'Second cluster')

#Printing the third cluster states
kable(rownames(USArrests)[usa_three == 3],
      col.names = 'States',
      caption = 'Third cluster')

```

<br></br>

(c) **Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.**

```{r}
#Scaling the variables
USArrests_scaled <- scale(USArrests)

usa_scaled <- hclust(dist(USArrests_scaled), method = "complete")

plot(usa_scaled)

```
<br></br>

(d) **What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.**
```{r}
#Plotting scaled and un-scaled tress
par(mfrow = c(1,2))
plot(usa_complete, main = 'Without scaling')
plot(usa_scaled, main= 'With scaling')

#Frequency tables for scaled and un-scaled clusters
kable(table(usa_three),
      col.names = c('Class', 'Frequency'),
      caption = 'Unscaled cluster')
kable(table(cutree(usa_scaled, 3)), 
      caption = 'Scaled cluster',
      col.names = c('Class', 'Frequency'))

```


The hierarchical clusters obtain with and without scaling are different. As seen in the frequency tables, scaling has affected the importance of each variable and therefore how each observation gets assigned into individual clusters. 

The features in the given data set have different measurement units, therefore the variables should be scaled before any further computations.

<br></br>

### 5. PCA and K-Means Clustering

(a) **Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.**
```{r}
#Generating the data
set.seed(1000)

mydata <- matrix(rnorm(60*50), ncol=50)

mydata[1:20,] <- mydata[1:20,] + 2
mydata[21:40,] <- mydata[21:40,]
mydata[21:40,] <- mydata[21:40,] - 2

plot(mydata,
     xlab = 'X values',
     ylab = 'Y values',
     main = 'Simulated data')
```
<br></br>

(b) **Perform PCA on the 60 observations and plot the first two principal components’ eigenvector. Use a different color to indicate the observations in each of the three classes.**
```{r}
#Performing PCA
mydata_pca <- prcomp(mydata, scale = FALSE)

plot(mydata_pca$x[,1], mydata_pca$x[,2], col = c(rep(3,20), rep(2,20), rep(1,20)),
     xlab = 'X values',
     ylab = 'Y values',
     main = 'PCA')
```
<br></br>


(c) **Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?**

```{r}
#Performing K-means, with K=3
mydata_kmeans_3 <- kmeans(mydata, 3, nstart = 50)
plot(mydata, col=(mydata_kmeans_3$cluster+1),
     xlab = 'X values',
     ylab = 'Y values',
     main = 'K-means, K=3')

kable(table(clustered = mydata_kmeans_3$cluster, 
            true_lables = c(rep(3,20), rep(2,20), rep(1,20))),
      caption = 'True labels and clusters',
      col.names = c('True Class 1', 'True Class 2', 'True Class 3'),
      row.names = TRUE)
```


The 3 true classes were clustered perfectly.

(d) **Perform K-means clustering with K = 2. Describe your results.**
```{r}
#Performing K-means, with K=2
mydata_kmeans_2 <- kmeans(mydata, 2, nstart = 50)
plot(mydata, col=(mydata_kmeans_2$cluster+1),
     xlab = 'X values',
     ylab = 'Y values',
     main = 'K-means, K=2')

kable(table(clustered = mydata_kmeans_2$cluster, 
            true_lables = c(rep(3,20), rep(2,20), rep(1,20))),
      caption = 'True labels and clusters',
      col.names = c('True Class 1', 'True Class 2', 'True Class 3'),
      row.names = TRUE)  
```

One of the clusters match the true labels, while the second cluster appears to have merged the two original labels into one cluster.

<br></br>


(e) **Now perform K-means clustering with K = 4, and describe your results.**
```{r}
#Performing K-means, with K=4
mydata_kmeans_4 <- kmeans(mydata, 4, nstart = 50)
plot(mydata, col=(mydata_kmeans_4$cluster+1),
     xlab = 'X values',
     ylab = 'Y values',
     main = 'K-means, K=4')

kable(table(clustered = mydata_kmeans_4$cluster, 
            true_lables = c(rep(3,20), rep(2,20), rep(1,20))),
      caption = 'True labels and clusters',
      col.names = c('True Class 1', 'True Class 2', 'True Class 3'),
      row.names = TRUE)
```

<br></br>

The additional cluster has split up one of the original labels into two distinct clusters. Other two clusters match the true labels perfectly.

<br></br>

(f) **Now perform K-means clustering with K = 3 on the first two principal components, rather than on the raw data.Comment on the results**
```{r}

mydata_pca_one_two <- data.frame(mydata_pca$x[,1], mydata_pca$x[,2])
mydata_kmeans_pca <- kmeans(mydata_pca_one_two, 3, nstart = 50)

plot(mydata_pca_one_two, col=(mydata_kmeans_pca$cluster+1),
     xlab = 'X values',
     ylab = 'Y values',
     main = 'K-means and principal components')

kable(table(clustered = mydata_kmeans_pca$cluster, 
            true_lables = c(rep(3,20), rep(2,20), rep(1,20))),
      caption = 'True labels and clusters',
      col.names = c('True Class 1', 'True Class 2', 'True Class 3'),
      row.names = TRUE)  

```

The clusters are matched up to true labels with no errors. 

<br></br>

(g) **Using the scale() function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. How do these results compare to the true class labels? Will the scaling affect the clustering?**
```{r}
mydata_scaled <- scale(mydata)

mydata_scale_kmeans <- kmeans(mydata_scaled, 3, nstart = 50)

plot(mydata_scaled, col=(mydata_scale_kmeans$cluster+1),
     xlab = 'X values',
     ylab = 'Y values',
     main = 'K-means, K = 3, scaled')

kable(table(clustered = mydata_scale_kmeans$cluster, 
            true_lables = c(rep(3,20), rep(2,20), rep(1,20))),
      caption = 'True labels and clusters',
      col.names = c('True Class 1', 'True Class 2', 'True Class 3'),
      row.names = TRUE)  
```

<br></br>

The true labels and clusters match up perfectly. Scaling did not have any effect as the data set did not contain any irregular measurement units. 
